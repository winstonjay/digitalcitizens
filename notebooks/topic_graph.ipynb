{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a graph from generated topics.\n",
    "\n",
    "Following generates json data for generating interactive topic graphs in JavaScript & HTML. The bulk of the code writtern here has been compressed into the file topic_graph for easier use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "df = pd.read_csv(\"../data/twitter/tweets_large_train\", na_filter=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating topics\n",
    "\n",
    "Will use data from a previous collection. Scikit-learn's Latent Dirchlet Allocation (LDA) will be used to generate topic's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, \n",
      "number of topics=5\n",
      "n_samples=40 and n_features=5000...\n",
      "time: 686.82 secs\n"
     ]
    }
   ],
   "source": [
    "n_features = 5000\n",
    "n_topics = 5\n",
    "n_samples = 40\n",
    "\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=10,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "\n",
    "tf_features = tf_vectorizer.fit_transform(df.text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"\\nnumber of topics=%d\\nn_samples=%d and n_features=%d...\"\n",
    "      % (n_topics, n_samples, n_features))\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=50,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "# this will take a while. reduce max_iter to reduce this.\n",
    "t0 = time.time()\n",
    "lda.fit(tf_features)\n",
    "print(\"time: {:.2f} secs\".format(time.time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the topics and their scores for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### topic: 0\n",
      "citizenship 10840.53, vote 4121.55, illeg 3711.21, immigr 3312.97, trump 2014.66, legal 1855.68, american 1809.00, daca 1588.47, path 1477.78, want 1362.27, democrat 1316.33, citizen 1309.40, ask 1232.94, voter 1189.20, deport 1163.20, million 1162.46, tax 1154.44, dreamer 1141.60, obama 1023.22, presid 975.45, parti 967.29, appli 963.35, russian 903.45, card 902.72, dem 898.76, \n",
      "#### topic: 1\n",
      "citizen 5721.96, state 3449.32, pleas 1948.39, trump 1720.42, today 1688.45, new 1605.87, privat 1576.76, offic 1446.70, general 1439.44, report 1346.30, presid 1311.44, unit 1292.26, attorney 1205.19, lawsuit 1202.77, major 1175.97, target 886.75, repres 840.77, polic 782.24, pull 755.54, join 736.55, district 733.31, press 723.57, associ 702.04, order 694.20, polici 653.45, \n",
      "#### topic: 2\n",
      "citizen 20187.92, gun 8951.24, right 7337.69, everi 4178.07, need 3909.13, weapon 3237.75, peopl 3102.05, protect 2908.98, arm 2716.46, govern 2522.37, american 2498.63, nra 2206.42, elect 2122.04, use 2054.92, constitut 1987.68, amend 1924.00, respons 1838.77, want 1803.04, vote 1732.22, state 1725.15, militari 1686.96, democraci 1674.74, assault 1604.64, pay 1539.33, averag 1450.36, \n",
      "#### topic: 3\n",
      "citizen 12845.48, citizenship 5558.12, countri 3100.15, eu 2024.01, work 1934.93, live 1921.12, like 1864.00, uk 1618.93, nation 1557.08, world 1382.26, year 1345.99, home 1338.17, make 1233.88, wrong 1223.20, right 1189.79, dual 1172.76, british 1167.26, born 1122.37, help 1112.98, peopl 1104.84, india 1103.59, student 1096.02, class 1093.81, govern 1069.05, american 1049.62, \n",
      "#### topic: 4\n",
      "citizen 15705.80, law 9108.68, abid 3941.33, like 3243.43, peopl 2348.52, know 2327.50, american 2218.06, think 1922.18, say 1887.65, make 1814.75, want 1766.21, good 1745.33, countri 1643.12, crimin 1588.76, senior 1586.41, right 1575.96, tri 1572.99, stop 1494.42, everi 1325.24, need 1296.35, becom 1232.02, yes 1200.49, come 1190.07, thing 1186.06, time 1165.03, \n"
     ]
    }
   ],
   "source": [
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    arr = []\n",
    "    print(\"#### topic:\", topic_idx)\n",
    "    for i in topic.argsort()[:-25 - 1:-1]:\n",
    "        print(\"{} {:.2f}\".format(\n",
    "            tf_feature_names[i], topic[i]), end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to format the data?\n",
    "\n",
    "I want this graph to be interactive in a browser so i will export it to json for use with `d3.js`. For this, looking at other peoples examples making graphs with d3.js it seems a common structure is to make two arrays one containing nodes the other containing links.\n",
    "\n",
    "It is going to be a bipartide graph, so the topic Nodes are connected by term Nodes this can be visualised as follows, where `t` is a term node and `O` is a term node:\n",
    "            \n",
    "         O   O   O\n",
    "         |    \\ /\n",
    "    O -- t     t - O\n",
    "          \\   /     \\  \n",
    "           \\ /       \\\n",
    "            O ------- t --- O\n",
    "                     / |\n",
    "                    /  |\n",
    "                   O   O\n",
    "\n",
    "\n",
    "Here is psuedo struct representing the each node object. The type property will designate wether is a topic node or a term node.\n",
    "\n",
    "    Node {\n",
    "        name:  String\n",
    "        root:  Bool\n",
    "    }\n",
    "    \n",
    "Here is psuedo struct representing the each link object.\n",
    "\n",
    "    Link {\n",
    "        source:     Int    \n",
    "        target:     Int\n",
    "        weight:     Float\n",
    "        group:      Int\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_graph(model, names, n_samples):\n",
    "    (links, nodes) = ([], [])\n",
    "    index = {}\n",
    "    index_n = 0 # keep track of the node indexes.\n",
    "    for i, topic in enumerate(model.components_):\n",
    "        topic_root = letters[i]\n",
    "        nodes.append(Node(topic_root, True))\n",
    "        index[topic_root] = index_n\n",
    "        index_n += 1\n",
    "        for j in topic.argsort()[:-n_samples - 1:-1]:\n",
    "            term = names[j]\n",
    "            if term not in index:\n",
    "                nodes.append(Node(term, False))\n",
    "                index[term] = max_n\n",
    "                index_n += 1\n",
    "            link = Link(index[topic_root], index[term], topic[j], i)\n",
    "            links.append(link)\n",
    "    return (nodes, norm_scale(links))\n",
    "        \n",
    "def Node(name, root):\n",
    "    \"pusedo Node struct constructor\"\n",
    "    return dict(name=name, root=root)\n",
    "\n",
    "def Link(source, target, weight, group):\n",
    "    \"pusedo Link struct constructor\"\n",
    "    return dict(source=source, \n",
    "                target=target,\n",
    "                weight=weight,\n",
    "                group=group)\n",
    "\n",
    "def norm_scale(x):\n",
    "    \"apply min max normalization\"\n",
    "    X = np.array([v['weight'] for v in x])\n",
    "    X = 1 + np.log(X) # apply sub-linear scaling\n",
    "    X = (X - X.min()) / (X.max() - X.min())\n",
    "    for v, w in zip(x, X):\n",
    "        v[\"weight\"] = w\n",
    "    return x\n",
    "\n",
    "letters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(nodes, links) = build_graph(lda, tf_feature_names, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': False, 'name': 'presid'}\n",
      "{'root': False, 'name': 'parti'}\n",
      "{'root': False, 'name': 'appli'}\n",
      "{'root': False, 'name': 'russian'}\n",
      "{'root': False, 'name': 'card'}\n",
      "{'root': False, 'name': 'dem'}\n",
      "{'root': False, 'name': 'question'}\n",
      "{'root': False, 'name': 'resid'}\n",
      "{'root': False, 'name': 'year'}\n",
      "{'root': False, 'name': 'provid'}\n"
     ]
    }
   ],
   "source": [
    "for n in nodes[20:30]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weight': 0.18311055383608046, 'source': 0, 'target': 21}\n",
      "{'weight': 0.18201426987995756, 'source': 0, 'target': 22}\n",
      "{'weight': 0.16475428002390025, 'source': 0, 'target': 23}\n",
      "{'weight': 0.16453694885339476, 'source': 0, 'target': 24}\n",
      "{'weight': 0.16335475354630319, 'source': 0, 'target': 25}\n",
      "{'weight': 0.16191834618213768, 'source': 0, 'target': 26}\n",
      "{'weight': 0.15762542530571419, 'source': 0, 'target': 27}\n",
      "{'weight': 0.15623092820349119, 'source': 0, 'target': 28}\n",
      "{'weight': 0.15251215849975455, 'source': 0, 'target': 29}\n",
      "{'weight': 0.13812132903575133, 'source': 0, 'target': 30}\n"
     ]
    }
   ],
   "source": [
    "for n in links[20:30]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 159\n"
     ]
    }
   ],
   "source": [
    "m = len(links)\n",
    "n = len(nodes)\n",
    "\n",
    "print(m, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved result to ../data/word_graph.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "out = \"../data/word_graph.json\"\n",
    "\n",
    "with open(out, \"w\") as f:\n",
    "    json.dump({\"nodes\": nodes,  \"links\": links}, \n",
    "              f, indent=4, sort_keys=True)\n",
    "print(\"saved result to\", out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
